{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a third kind of feature that can be found in many applications, which is **text**.\n",
    "\n",
    "Text data is usually represented as **strings**, made up of characters. In any of the examples just given, the length of the text data will vary. This feature is clearly very differ‐\n",
    "ent from the numeric features and we need to process the data before we can apply our machine learning algorithms to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Data Represented As Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is usually just a **string** in your dataset, but not all string features should be treated as text. A string feature can sometimes represent categorical variables. There is no way to know how to treat a string feature before looking at the data.\n",
    "\n",
    "There are four kinds of string data you might see:\n",
    "* Categorical data\n",
    "* Free strings that can be semantically mapped to categories\n",
    "* Structured string data\n",
    "* Text data\n",
    "\n",
    "**Categorical data** is data that comes from a **fixed list**. Ex : [red,green,blue]. You can check whether this is the case for your data by eyeballing it and confirm it by computing the unique values over the dataset, and possibly a histogram over how often each appears. \n",
    "\n",
    "**free strings that can be semantically mapped to categories** are for example the responses that can be obtain from a text field. It will probably be best to encode this data as a categorical variable, where you can select the categories either by using the most common entries, or by defining categories that will capture responses in a way that makes sense for your application. This kind of preprocessing of strings can take a lot of manual effort and is not easily automated. If you are in a position where you can influence data collection, we highly recommend avoiding manually entered values for concepts that are better captured using categorical variables.\n",
    "\n",
    "**Structured string data** : manually entered values that do not correspond to fixed categories, but still have some underlying structure, like addresses, names of places or people, dates, telephone. These kinds of strings are often very hard to parse, and their treatment is highly dependent on context and domain.\n",
    "\n",
    "**Text data** : consists of phrases or sentences. Ex : Tweets, chats, reviews.. In the context of text analysis, the dataset is often called the **corpus**, and each data point, represented as a single text, is called a **document**. These terms come from the information retrieval (IR) and natural language processing (NLP) community, which both deal mostly in text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Application : Sentiment Analysis of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains the text of the reviews, together with a label that indicates whether a review is “positive” or “negative\". After unpacking the data, the dataset is provided as text files in two separate folders, one for the training data and one for the test data. Each of these in turn has two subfolders, one called pos and one called neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "# Ignorer les avertissements spécifiques liés à la convergence\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 75000\n",
      "text_train[1]:\n",
      "b\"Amount of disappointment I am getting these days seeing movies like Partner, Jhoom Barabar and now, Heyy Babyy is gonna end my habit of seeing first day shows.<br /><br />The movie is an utter disappointment because it had the potential to become a laugh riot only if the d\\xc3\\xa9butant director, Sajid Khan hadn't tried too many things. Only saving grace in the movie were the last thirty minutes, which were seriously funny elsewhere the movie fails miserably. First half was desperately been tried to look funny but wasn't. Next 45 minutes were emotional and looked totally artificial and illogical.<br /><br />OK, when you are out for a movie like this you don't expect much logic but all the flaws tend to appear when you don't enjoy the movie and thats the case with Heyy Babyy. Acting is good but thats not enough to keep one interested.<br /><br />For the positives, you can take hot actresses, last 30 minutes, some comic scenes, good acting by the lead cast and the baby. Only problem is that these things do not come together properly to make a good movie.<br /><br />Anyways, I read somewhere that It isn't a copy of Three men and a baby but I think it would have been better if it was.\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[1]:\\n{}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500 50000]\n"
     ]
    }
   ],
   "source": [
    "# Review contains HTML line breaks. Clean the data\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task we want to solve is as follows: given a review, we want to assign the label “positive” or “negative” based on the text content of the review. This is a standard\n",
    "binary classification task. However, the text data is not in a format that a machine learning model can handle. We need to convert the string representation of the text\n",
    "into a numeric representation that we can apply our machine learning algorithms to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Text Data as a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most simple but effective and commonly used ways to represent text for machine learning is using the **bag-of-words representation**. When using this representation, we discard most of the structure of the input text, like chapters, paragraphs, sentences, and formatting, and **only count how often each word appears** in each text in the corpus.\n",
    "\n",
    "Computing the bag-of-words representation for a corpus of documents consists of the following three steps:\n",
    "1. **Tokenization.** Split each document into the words that appear in it (called tokens), for example by splitting them on whitespace and punctuation.\n",
    "2. **Vocabulary building**. Collect a vocabulary of all words that appear in any of the documents, and number them (say, in alphabetical order).\n",
    "3. **Encoding.** For each document, count how often each of the words in the vocabulary appear in this document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Bag of Words to a toy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words representation is implemented in **CountVectorizer**, which is a transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "bards_words =[\"The fool doth think he is wise,\",\n",
    " \"but the wise man knows himself to be a fool\"]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n",
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words)))\n",
    "print(\"Dense representation of bag_of_words:\\n{}\".format(\n",
    " bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words for Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<75000x124255 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 10315542 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 124255\n",
      "First 20 features:\n",
      "['00' '000' '0000' '0000000000000000000000000000000001' '0000000000001'\n",
      " '000000001' '000000003' '00000001' '000001745' '00001' '0001' '00015'\n",
      " '0002' '0007' '00083' '000ft' '000s' '000th' '001' '002']\n",
      "Features 20010 to 20030:\n",
      "['cheapen' 'cheapened' 'cheapening' 'cheapens' 'cheaper' 'cheapest'\n",
      " 'cheapie' 'cheapies' 'cheapjack' 'cheaply' 'cheapness' 'cheapo'\n",
      " 'cheapozoid' 'cheapquels' 'cheapskate' 'cheapskates' 'cheapy' 'chearator'\n",
      " 'cheat' 'cheata']\n",
      "Every 2000th feature:\n",
      "['00' '_require_' 'aideed' 'announcement' 'asteroid' 'banquière'\n",
      " 'besieged' 'bollwood' 'btvs' 'carboni' 'chcialbym' 'clotheth'\n",
      " 'consecration' 'cringeful' 'deadness' 'devagan' 'doberman' 'duvall'\n",
      " 'endocrine' 'existent' 'fetiches' 'formatted' 'garard' 'godlie' 'gumshoe'\n",
      " 'heathen' 'honoré' 'immatured' 'interested' 'jewelry' 'kerchner' 'köln'\n",
      " 'leydon' 'lulu' 'mardjono' 'meistersinger' 'misspells' 'mumblecore'\n",
      " 'ngah' 'oedpius' 'overwhelmingly' 'penned' 'pleading' 'previlage'\n",
      " 'quashed' 'recreating' 'reverent' 'ruediger' 'sceme' 'settling'\n",
      " 'silveira' 'soderberghian' 'stagestruck' 'subprime' 'tabloids' 'themself'\n",
      " 'tpf' 'tyzack' 'unrestrained' 'videoed' 'weidler' 'worrisomely'\n",
      " 'zombified']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try to improve our feature extraction, let’s obtain a quantitative measure of performance by actually building a classifier. We have the training labels stored in\n",
    "y_train and the bag-of-words representation of the training data in X_train, so we can train a classifier on this data. For high-dimensional, sparse data like this, linear\n",
    "models like LogisticRegression often work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.68\n",
      "Best parameters:  {'C': 0.01}\n",
      "0.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "X_test = vect.transform(text_test)\n",
    "print(\"{:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s see if we can improve the extraction of words. The CountVectorizer extracts tokens using a regular expression. By default, the regular expression that is used is **\"\\b\\w\\w+\\b\"**. This means it finds all sequences of characters that consist of at least two letters or numbers (\\w) and that are separated by word boundaries (\\b). It does not find single-letter words, and it splits up contractions like “doesn’t” or “bit.ly”, but it matches “h8ter” as a single word. \n",
    "\n",
    "The CountVectorizer then converts all words to lowercase characters, so that “soon”, “Soon”, and “sOon” all correspond to the same token (and therefore feature).\n",
    "This simple mechanism works quite well in practice, but as we saw earlier, we get many uninformative features (like the numbers). One way to cut back on these is to\n",
    "only use tokens that appear in at least two documents (or at least five documents, and so on). A token that appears only in a single document is unlikely to appear in the test\n",
    "set and is therefore not helpful. We can set the minimum number of documents a token needs to appear in with the min_df parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <75000x44532 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 10191240 stored elements in Compressed Sparse Row format>\n",
      "First 50 features:\n",
      "['00' '000' '001' '007' '00am' '00pm' '00s' '01' '02' '03' '04' '05' '06'\n",
      " '07' '08' '09' '10' '100' '1000' '1001' '100k' '100th' '100x' '101'\n",
      " '101st' '102' '103' '104' '105' '106' '107' '108' '109' '10am' '10pm'\n",
      " '10s' '10th' '10x' '11' '110' '1100' '110th' '111' '112' '1138' '115'\n",
      " '116' '117' '11pm' '11th']\n",
      "Features 20010 to 20030:\n",
      "['inert' 'inertia' 'inescapable' 'inescapably' 'inevitability'\n",
      " 'inevitable' 'inevitably' 'inexcusable' 'inexcusably' 'inexhaustible'\n",
      " 'inexistent' 'inexorable' 'inexorably' 'inexpensive' 'inexperience'\n",
      " 'inexperienced' 'inexplicable' 'inexplicably' 'inexpressive'\n",
      " 'inextricably']\n",
      "Every 700th feature:\n",
      "['00' 'accountability' 'alienate' 'appetite' 'austen' 'battleground'\n",
      " 'bitten' 'bowel' 'burton' 'cat' 'choreographing' 'collide' 'constipation'\n",
      " 'creatively' 'dashes' 'descended' 'dishing' 'dramatist' 'ejaculation'\n",
      " 'epitomize' 'extinguished' 'figment' 'forgot' 'garnished' 'goofy' 'gw'\n",
      " 'hedy' 'hormones' 'imperfect' 'insomniac' 'janitorial' 'keira' 'lansing'\n",
      " 'linfield' 'mackendrick' 'masterworks' 'miao' 'moorehead' 'natassia'\n",
      " 'nude' 'ott' 'particulars' 'phillipines' 'pop' 'profusely' 'raccoons'\n",
      " 'redolent' 'responding' 'ronno' 'satirist' 'seminal' 'shrews' 'smashed'\n",
      " 'spendthrift' 'stocked' 'superman' 'tashman' 'tickets' 'travelling'\n",
      " 'uncomfortable' 'uprising' 'vivant' 'whine' 'x2']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))\n",
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.68\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way that we can **get rid of uninformative words** is by discarding words that are **too frequent to be informative**. \n",
    "\n",
    "There are two main approaches: using a l**anguagespecific list of stopwords**, or **discarding words that appear too frequently**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword:\n",
      "['onto', 'hereupon', 'enough', 'on', 'take', 'even', 'top', 'bottom', 'herein', 'any', 'how', 'thru', 'twenty', 'show', 'so', 'here', 'cant', 'with', 'why', 'and', 'eleven', 'until', 'she', 'amoungst', 'whole', 'down', 'someone', 'one', 'becomes', 'couldnt', 'must', 'therefore']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Caching the list of root modules, please wait!\n",
      "(This will only be done once - type '%rehashx' to reset cache!)\n",
      "\n",
      "X_train with stop words:\n",
      "<75000x44223 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 6577418 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Specifying stop_words=\"english\" uses the built-in list.\n",
    "# We could also augment it and pass our own.\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.71\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling the Data with tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of dropping features that are deemed unimportant, another approach is to **rescale features by how informative we expect them to be**. One of the most common\n",
    "ways to do this is using the **term frequency–inverse document frequency (tf–idf)** method. \n",
    "\n",
    "The intuition of this method is to give high weight to any term that appears often in a particular document, but not in many documents in the corpus. If a word appears often in a particular document, but not in very many documents, it is likely to be very descriptive of the content of that document.\n",
    "\n",
    "**TfidfTransformer**, which takes in the sparse matrix output produced by CountVectorizer and transforms it, and **TfidfVectorizer**, which takes in the text data and does both the bag-of-words feature extraction and the tf–idf transformation\n",
    "\n",
    "The tf–idf score for word w in document d as implemented in both the TfidfTransformer and TfidfVectorizer classes is given by:\n",
    "\n",
    "$$\n",
    "w_{d} = tf \\times \\log\\left(\\frac{N + 1}{N_w + 1}\\right) + 1\n",
    "$$\n",
    "\n",
    "where N is the number of documents in the training set, Nw is the number of docu‐\n",
    "ments in the training set that the word w appears in, and tf (the term frequency) is the\n",
    "number of times that the word w appears in the query document d (the document\n",
    "you want to transform or encode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n",
    " LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['remained' 'acclaimed' 'combines' 'rapidly' 'uniformly' 'diverse'\n",
      " 'avoiding' 'fills' 'feeble' 'admired' 'wherever' 'admission' 'abound'\n",
      " 'starters' 'assure' 'pivotal' 'comprehend' 'deliciously' 'strung'\n",
      " 'inadvertently']\n",
      "Features with highest tfidf: \n",
      "['nukie' 'reno' 'dominick' 'taz' 'ling' 'rob' 'victoria' 'turtles'\n",
      " 'khouri' 'lorenzo' 'id' 'zizek' 'elwood' 'nikita' 'rishi' 'timon'\n",
      " 'titanic' 'zohan' 'pammy' 'godzilla']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "# transform the training dataset\n",
    "X_train = vectorizer.transform(text_train)\n",
    "# find maximum value for each of the features over the dataset\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "# get feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    " feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"Features with highest tfidf: \\n{}\".format(\n",
    " feature_names[sorted_by_tfidf[-20:]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with low tf–idf are those that either are very commonly used across docu‐\n",
    "ments or are only used sparingly, and only in very long documents. Interestingly,\n",
    "many of the high-tf–idf features actually identify certain shows or movies. These words are unlikely to help us in our sentiment classification task\n",
    "but certainly contain a lot of specific information about the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    " feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these are mostly English stopwords like \"the\" and \"no\". But some are clearly domain-specific to the movie reviews, like \"movie\", \"film\", \"time\", \"story\",\n",
    "and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Model Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are so many features—27,271 after removing the infrequent ones—we clearly cannot look at all of the coefficients at the same time.\n",
    "However, we can look at the largest coefficients, and see which words these correspond to. We will use the last model that we trained, based on the tf–idf feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.tools.visualize_coefficients(\n",
    " grid.best_estimator_.named_steps[\"logisticregression\"].coef_,\n",
    " feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative coefficients on the left belong to words that according to the model are indicative of negative reviews, while the positive coefficients on the right belong to\n",
    "words that according to the model indicate positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words with More Than One Word (n-Grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main disadvantages of using a bag-of-words representation is that word order is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n",
    "“it’s good, not bad at all” have exactly the same representation, even though the meanings are inverted. Putting “not” in front of a word is only one example (if an extreme\n",
    "one) of how context matters. \n",
    "\n",
    "Fortunately, there is a way of capturing context when using a bag-of-words representation, by not only considering the counts of single tokens, but also the counts of pairs or triplets of tokens that appear next to each other.\n",
    "\n",
    "Pairs of tokens are known as **bigrams**, triplets of tokens are known as **trigrams**, and more generally sequences of tokens are known as **n-grams**. We can change the range\n",
    "of tokens that are considered as features by changing the ngram_range parameter of CountVectorizer or TfidfVectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unigrams\n",
    "print(\"bards_words:\\n{}\".format(bards_words))\n",
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams\n",
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed data (dense):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most applications, the minimum number of tokens should be one, as single words often capture a lot of meaning. Adding bigrams helps in most cases. Adding\n",
    "longer sequences—up to 5-grams—might help too, but this will lead to an explosion of the number of features and might lead to overfitting, as there will be many very\n",
    "specific features. In principle, the number of bigrams could be the number of unigrams squared and the number of trigrams could be the number of unigrams to\n",
    "the power of three, leading to very large feature spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "# running the grid search takes a long time because of the\n",
    "# relatively large grid and the inclusion of trigrams\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    " \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# extract scores from grid_search\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n",
    "# visualize heat map\n",
    "heatmap = mglearn.tools.heatmap(\n",
    " scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n",
    " xticklabels=param_grid['logisticregression__C'],\n",
    " yticklabels=param_grid['tfidfvectorizer__ngram_range'])\n",
    "plt.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature names and coefficients\n",
    "vect = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "coef = grid.best_estimator_.named_steps['logisticregression'].coef_\n",
    "mglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are particularly interesting features containing the word “worth” that were not\n",
    "present in the unigram model: \"not worth\" is indicative of a negative review, while\n",
    "\"definitely worth\" and \"well worth\" are indicative of a positive review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 3-gram features\n",
    "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n",
    "# visualize only 3-gram features\n",
    "mglearn.tools.visualize_coefficients(coef.ravel()[mask],\n",
    " feature_names[mask], n_top_features=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Tokenization, Stemming, and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particular step that is often improved in more sophisticated text-processing applications is the first step in the bag-of-words model: **tokenization**. This step defines what\n",
    "constitutes a word for the purpose of feature extraction.\n",
    "\n",
    "We saw earlier that the vocabulary often contains singular and plural versions of some words. For the purposes of a bag-of-words model, the semantics\n",
    "of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only increase overfitting, and not allow the model to fully exploit the training data. Similarly to having singular and plural forms of a noun, treating different verb forms and related words as distinct tokens is disadvantageous for building a model that generalizes well.\n",
    "\n",
    "This problem can be overcome by representing each word using its word **stem**, which involves identifying (or conflating) all the words that have the same word stem. If this\n",
    "is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as **stemming**. \n",
    "\n",
    "If instead a dictionary of known word forms is used (**an explicit and human-verified system**), and the role of the word in the sentence is taken into account, the process is referred to as **lemmatization** and the standardized form of the word is referred to as the **lemma**. \n",
    "\n",
    "Both processing methods, lemmatization and stemming, are forms of normalization that try to extract some normal form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "# load spacy's English-language models\n",
    "en_nlp = spacy.load('en')\n",
    "# instantiate nltk's Porter stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "# define function to compare lemmatization in spacy with stemming in nltk\n",
    "def compare_normalization(doc):\n",
    " # tokenize document in spacy\n",
    " doc_spacy = en_nlp(doc)\n",
    " # print lemmas found by spacy\n",
    " print(\"Lemmatization:\")\n",
    " print([token.lemma_ for token in doc_spacy])\n",
    " # print tokens found by Porter stemmer\n",
    " print(\"Stemming:\")\n",
    " print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_normalization(u\"Our meeting today was worse than yesterday, \"\n",
    " \"I'm scared of meeting the clients tomorrow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, lemmatization is a much more involved process than stemming, but it usually produces better results than stemming when used for normalizing tokens for machine learning.\n",
    "\n",
    "While scikit-learn implements neither form of normalization, CountVectorizer\n",
    "allows specifying your own tokenizer to convert each document into a list of tokens\n",
    "using the tokenizer parameter. We can use the lemmatization from spacy to create a\n",
    "callable that will take a string and produce a list of lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technicality: we want to use the regexp-based tokenizer\n",
    "# that is used by CountVectorizer and only use the lemmatization\n",
    "# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n",
    "# with the regexp-based tokenization.\n",
    "import re\n",
    "# regexp used in CountVectorizer\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "# load spacy language model and save old tokenizer\n",
    "en_nlp = spacy.load('en')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "# replace the tokenizer with the preceding regexp\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
    " regexp.findall(string))\n",
    "# create a custom tokenizer using the spacy document processing pipeline\n",
    "# (now using our own tokenizer)\n",
    "def custom_tokenizer(document):\n",
    " doc_spacy = en_nlp(document, entity=False, parse=False)\n",
    " return [token.lemma_ for token in doc_spacy]\n",
    "# define a count vectorizer with the custom tokenizer\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\n",
    "\n",
    "# transform text_train using CountVectorizer with lemmatization\n",
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n",
    "# standard CountVectorizer for reference\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a grid search using only 1% of the data as the training set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "cv = StratifiedShuffleSplit(n_iter=5, test_size=0.99,\n",
    " train_size=0.01, random_state=0)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
    "# perform grid search with standard CountVectorizer\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score \"\n",
    " \"(standard CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
    "# perform grid search with lemmatization\n",
    "grid.fit(X_train_lemma, y_train)\n",
    "print(\"Best cross-validation score \"\n",
    " \"(lemmatization): {:.3f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling and Document Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particular technique that is often applied to text data is **topic modeling**, which is an umbrella term describing the task of assigning each document to one or multiple topics, usually without supervision.\n",
    "\n",
    "Often, when people talk about topic modeling, they refer to one particular decomposition method called **Latent Dirichlet Allocation (often LDA for short)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, the LDA model tries to find groups of words (the topics) that appear together frequently. LDA also requires that each document can be understood as a\n",
    "“mixture” of a subset of the topics. It is important to understand that for the machine learning model a “topic” might not be what we would normally call a topic in every‐\n",
    "day speech, but that it resembles more the components extracted by PCA or NMF .Even if there is a semantic meaning for an LDA “topic”, it might not be some‐\n",
    "thing we’d usually call a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=10, learning_method=\"batch\",\n",
    " max_iter=25, random_state=0)\n",
    "# We build the model and transform the data in one step\n",
    "# Computing transform takes some time,\n",
    "# and we can save time by doing both at  once\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic (a row in the components_), sort the features (ascending)\n",
    "# Invert rows with [:, ::-1] to make sorting descending\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "# Print out the 10 topics:\n",
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n",
    " sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the important words, topic 1 seems to be about historical and war mov‐\n",
    "ies, topic 2 might be about bad comedies, topic 3 might be about TV series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\n",
    " max_iter=25, random_state=0)\n",
    "document_topics100 = lda100.fit_transform(X)\n",
    "topics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\n",
    "sorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n",
    " sorting=sorting, topics_per_chunk=7, n_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics we extracted this time seem to be more specific, though many are hard to\n",
    "interpret.\n",
    "\n",
    "For example, topic 45 seems to be about music. Let’s check which kinds of\n",
    "reviews are assigned to this topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by weight of \"music\" topic 45\n",
    "music = np.argsort(document_topics100[:, 45])[::-1]\n",
    "# print the five documents where the topic is most important\n",
    "for i in music[:10]:\n",
    " # pshow first two sentences\n",
    " print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "topic_names = [\"{:>2} \".format(i) + \" \".join(words)\n",
    " for i, words in enumerate(feature_names[sorting[:, :2]])]\n",
    "# two column bar chart:\n",
    "for col in [0, 1]:\n",
    " start = col * 50\n",
    " end = (col + 1) * 50\n",
    " ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end])\n",
    " ax[col].set_yticks(np.arange(50))\n",
    " ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\n",
    " ax[col].invert_yaxis()\n",
    " ax[col].set_xlim(0, 2000)\n",
    " yax = ax[col].get_yaxis()\n",
    " yax.set_tick_params(pad=130)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models like LDA are interesting methods to understand large text corpora in the absence of labels—or, as here, even if labels are available. The LDA algorithm is\n",
    "randomized, though, and changing the random_state parameter can lead to quite different outcomes. While identifying topics can be helpful, any conclusions you\n",
    "draw from an unsupervised model should be taken with a grain of salt, and we rec‐\n",
    "ommend verifying your intuition by looking at the documents in a specific topic. The\n",
    "topics produced by the LDA.transform method can also sometimes be used as a com‐\n",
    "pact representation for supervised learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
